Istio通过结合连接池和实例健康检测机制实现熔断功能。当后端实例出现故障时就移出负载均衡池，当负载均衡池中无可用健康实例时，服务请求会立即得到服务不可用的响应码，此时服务就处于熔断状态了。当服务实例被移出的时间结束后，服务实例会被再次添加到负载均衡池中，等待下一轮的服务健康检测。

配置示例：

```yaml
 1 apiVersion：networking.istio.io/v1alpha3
 2 kind: DestinationRule
 3 metadata:
 4   name: service-go
 5 spec:
 6   host: service-go
 7   trafficPolicy:
 8     connectionPool:
 9       tcp:
10         maxConnections: 10
11       http:
12         http2MaxRequests: 10
13         maxRequestsPerConnection: 10
14     outlierDetection:
15       consecutiveErrors: 3
16       interval: 3s
17       baseEjectionTime: 3m
18       maxEjectionPercent: 100
19   subsets:
20   - name: v1
21     labels:
22       version: v1
23   - name: v2
24     labels:
25       version: v2
```
第8~13行定义了连接池配置，并发请求设置为10。

第14~18行定义了后端实例健康检测配置，允许全部实例移出连接池。

【实验】

1）启动用于并发测试的Pod：

```bash
kubectl apply -f kubernetes/fortio.yaml
```

2）创建service-go服务的路由规则：

```bash
kubectl apply -f istio/route/virtual-service-go.yaml
```

3）创建熔断规则：

```bash
kubectl apply -f istio/resilience/destination-rule-go-cb.yaml
```

4）访问：

```bash
[root@vms30 istio-lab]# kubectl exec fortio -c fortio /usr/local/bin/fortio -- load -curl http://service-go/env
HTTP/1.1 200 OK
content-type: application/json; charset=utf-8
date: Sat, 16 May 2020 02:54:20 GMT
content-length: 19
x-envoy-upstream-service-time: 5
server: envoy

```

```bash
[root@vms30 istio-lab]#  kubectl exec fortio -c fortio /usr/local/bin/fortio -- load -c 20 -qps 0 -n 200 -loglevel Error http://service-go/env
02:55:31 I logger.go:97> Log level is now 4 Error (was 2 Info)
Fortio 1.0.1 running at 0 queries per second, 1->1 procs, for 200 calls: http://service-go/env
Aggregated Function Time : count 200 avg 0.030616202 +/- 0.02123 min 0.0027694 max 0.114223187 sum 6.1232405
# target 50% 0.025
# target 75% 0.0379167
# target 90% 0.0583333
# target 99% 0.1
# target 99.9% 0.112801
Sockets used: 28 (for perfect keepalive, would be 20)
Code 200 : 192 (96.0 %)
Code 503 : 8 (4.0 %)
All done 200 calls (plus 0 warmup) 30.616 ms avg, 482.7 qps
```

```bash
[root@vms30 istio-lab]# kubectl exec fortio -c fortio /usr/local/bin/fortio -- load -c 30 -qps 0 -n 300 -loglevel Error http://service-go/env
02:57:15 I logger.go:97> Log level is now 4 Error (was 2 Info)
Fortio 1.0.1 running at 0 queries per second, 1->1 procs, for 300 calls: http://service-go/env
Aggregated Function Time : count 300 avg 0.024973717 +/- 0.02297 min 0.000371422 max 0.132712986 sum 7.49211507
# target 50% 0.0174667
# target 75% 0.0278378
# target 90% 0.0536364
# target 99% 0.125085
# target 99.9% 0.13195
Sockets used: 121 (for perfect keepalive, would be 30)
Code 200 : 204 (68.0 %)
Code 503 : 96 (32.0 %)
All done 300 calls (plus 0 warmup) 24.974 ms avg, 929.3 qps

```

```bash
[root@vms30 istio-lab]# kubectl exec fortio -c fortio /usr/local/bin/fortio -- load -c 40 -qps 0 -n 400 -loglevel Error http://service-go/env
02:58:12 I logger.go:97> Log level is now 4 Error (was 2 Info)
Fortio 1.0.1 running at 0 queries per second, 1->1 procs, for 400 calls: http://service-go/env
Aggregated Function Time : count 400 avg 0.027188206 +/- 0.03239 min 0.00108232 max 0.23720534 sum 10.8752825
# target 50% 0.0189677
# target 75% 0.03
# target 90% 0.0403846
# target 99% 0.215945
# target 99.9% 0.235079
Sockets used: 183 (for perfect keepalive, would be 40)
Code 200 : 240 (60.0 %)
Code 503 : 160 (40.0 %)
All done 400 calls (plus 0 warmup) 27.188 ms avg, 950.4 qps
```

实验部署了两个版本的service-go服务实例，每个版本一个Pod，每个Pod的并发数为10，所以总的最大并发数就为20。从压测结果可以看出，当并发逐渐增大时，服务不可用的响应码（503）所占比例逐渐升高。但是从结果看，熔断器并不是非常准确地拦截了高于设置并发值的请求，Istio允许有部分请求遗漏。

5）清理：

```bash
kubectl delete -f kubernetes/fortio.yaml
kubectl delete -f istio/route/virtual-service-go.yaml
kubectl delete -f istio/resilience/destination-rule-go-cb.yaml
```
